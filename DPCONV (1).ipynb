{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "ZTvyGNmGKyh0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class RE_BN_DPCONV(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, dirate=1, stride=1):\n",
        "        super(RE_BN_DPCONV, self).__init__()\n",
        "        # kernel 1X1\n",
        "        self.conv1x1= nn.Conv2d(in_ch, out_ch, 1)\n",
        "        # global average pooling\n",
        "        self.GAP =  nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #MLP\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear = nn.Linear(out_ch, 4)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "        # Paramidal convolution\n",
        "        self.conv3x3 = nn.Conv2d(out_ch, out_ch, 3, padding=1 * dirate, dilation=1 * dirate, stride=stride)\n",
        "        self.conv5x5 = nn.Conv2d(out_ch, out_ch, 5, padding=2 * dirate, dilation=1 * dirate, stride=stride)\n",
        "        self.conv7x7 = nn.Conv2d(out_ch, out_ch, 7, padding=3 * dirate, dilation=1 * dirate, stride=stride)\n",
        "        self.conv9x9 = nn.Conv2d(out_ch, out_ch, 9, padding=4 * dirate, dilation=1 * dirate, stride=stride)\n",
        "\n",
        "        # kernel 1X1 final\n",
        "        self.conv1x1_final= nn.Conv2d(4 * out_ch, out_ch, 1)\n",
        "\n",
        "        # ReLU + BatchNorm\n",
        "        self.bn_s1 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu_s1 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x): \n",
        "        x = self.conv1x1(x) \n",
        "        hx = x \n",
        "        # GAP + MLP\n",
        "        hx = self.GAP(hx)\n",
        "        hx = self.flatten(hx)\n",
        "        hx = self.relu(hx)\n",
        "        hx = self.linear(hx)\n",
        "        hx = self.softmax(hx)\n",
        "        # Paramidal convolution\n",
        "        out1 = self.conv3x3(x)\n",
        "        out2 = self.conv5x5(x)\n",
        "        out3 = self.conv7x7(x)\n",
        "        out4 = self.conv9x9(x)\n",
        "        \n",
        "        # channel wise\n",
        "        for i in range(hx.shape[0]):\n",
        "            out1[i] = out1[i] * hx[i][0]\n",
        "            out2[i] = out2[i] * hx[i][1]\n",
        "            out3[i] = out3[i] * hx[i][2]\n",
        "            out4[i] = out4[i] * hx[i][3]\n",
        "        # concatenation\n",
        "        out_fuse = torch.cat((out1, out2, out3, out4),1)\n",
        "        # conv1x1_final\n",
        "        y = self.conv1x1_final(out_fuse)\n",
        "        \n",
        "        return self.relu_s1(self.bn_s1(y + x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn((10, 30, 320, 320))\n",
        "model = RE_BN_DPCONV(in_ch=30, out_ch=15, dirate=1, stride=1)\n",
        "a = model(X)\n",
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL3Y0K7nNysF",
        "outputId": "932b4c13-ff8f-475a-cab6-576345849bef"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 15, 320, 320])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7grCHcsefmyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}